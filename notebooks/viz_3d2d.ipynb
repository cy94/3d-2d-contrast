{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c52d06-8487-4a80-b3be-56b77baf18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198dc03e-1bb9-4a00-871e-1e5807b6cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import trimesh\n",
    "import pyrender\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import os, os.path as osp\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import plot_matches\n",
    "import cv2\n",
    "\n",
    "from datasets.scannet.utils_3d import ProjectionHelper, adjust_intrinsic, make_intrinsic, load_intrinsic, load_pose\n",
    "from datasets.scannet.utils_3d import load_depth, load_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3294419-1a37-4905-a6c8-e055984bbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scan_name(scene_id, scan_id):\n",
    "    return f'scene{str(scene_id).zfill(4)}_{str(scan_id).zfill(2)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80458c51-6694-4050-b364-dca72631c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "subvol_size = (32, 32, 32)\n",
    "voxel_size = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516e802-57e6-4f88-a396-1376265e06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/mnt/data/scannet/backproj')\n",
    "fname = 'train100-v2-aug.h5'\n",
    "f = h5py.File(data_dir / fname, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac40dc5-356e-4f37-976d-0f6e6228fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.keys())\n",
    "print(f['x'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115cbdb-ae72-48bf-a12f-2c98366389f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndx =  5\n",
    "w2g, sceneid, scanid, frames = f['world_to_grid'][ndx], f['scene_id'][ndx], f['scan_id'][ndx], f['frames'][ndx]\n",
    "print(w2g)\n",
    "print(sceneid, scanid)\n",
    "print(frames)\n",
    "g2w = torch.inverse(torch.Tensor(w2g)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922971f-7906-4ac3-a800-ea1cfb10902c",
   "metadata": {},
   "source": [
    "## Draw only the scene and camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8de3ef-91d7-4b8a-921c-b2d0f60d511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = trimesh.scene.scene.Scene() \n",
    "\n",
    "# basics\n",
    "scan_name = get_scan_name(sceneid, scanid)\n",
    "print(scan_name)\n",
    "root = Path('/mnt/data/scannet/scans')\n",
    "\n",
    "# camera and frustum\n",
    "intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], (40, 30))\n",
    "\n",
    "pose_path = root / scan_name / 'pose' / f'{frames[0]}.txt'\n",
    "pose = load_pose(pose_path).numpy()\n",
    "focal = (intrinsic[0, 0], intrinsic[1, 1])\n",
    "cam = trimesh.scene.Camera(name=f'{scan_name}_{frames[0]}', resolution=(40, 30), focal=focal, z_near=0.4, z_far=4.0) \n",
    "\n",
    "camball, campath = trimesh.creation.camera_marker(cam, marker_height=4, origin_size=0.05)\n",
    "camball.apply_transform(pose)\n",
    "campath.apply_transform(pose)\n",
    "campath.colors = np.ones((5, 3)) * np.array((0, 255, 0))\n",
    "print('Add cam')\n",
    "scene.add_geometry(camball)\n",
    "scene.add_geometry(campath)\n",
    "\n",
    "# the scene mesh\n",
    "scan_path = root / f'{scan_name}/{scan_name}_vh_clean_2.ply'\n",
    "print(scan_path)\n",
    "scan = trimesh.load(scan_path)\n",
    "print(scan)\n",
    "print('Add scan')\n",
    "scene.add_geometry(scan)\n",
    "\n",
    "# axes\n",
    "axes = trimesh.creation.axis(axis_radius=0.1, axis_length=10)\n",
    "print('Add axes')\n",
    "scene.add_geometry(axes)\n",
    "\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b85ad9-4c89-40fc-bf04-f97aa2a250f9",
   "metadata": {},
   "source": [
    "## Draw one chunk and its corresponding camera pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926a3d2-8f6e-4fdf-b6eb-c9acca44ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scene = trimesh.scene.scene.Scene() \n",
    "\n",
    "# basics\n",
    "scan_name = get_scan_name(sceneid, scanid)\n",
    "print(scan_name)\n",
    "root = Path('/mnt/data/scannet/scans')\n",
    "\n",
    "# camera and frustum\n",
    "intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], (40, 30))\n",
    "\n",
    "pose_path = root / scan_name / 'pose' / f'{frames[0]}.txt'\n",
    "pose = load_pose(pose_path).numpy()\n",
    "focal = (intrinsic[0, 0], intrinsic[1, 1])\n",
    "cam = trimesh.scene.Camera(name=f'{scan_name}_{frames[0]}', resolution=(40, 30), focal=focal, z_near=0.4, z_far=4.0) \n",
    "\n",
    "camball, campath = trimesh.creation.camera_marker(cam, marker_height=4, origin_size=0.05)\n",
    "camball.apply_transform(pose)\n",
    "campath.apply_transform(pose)\n",
    "campath.colors = np.ones((5, 3)) * np.array((0, 255, 0))\n",
    "print('Add cam')\n",
    "scene.add_geometry(camball)\n",
    "scene.add_geometry(campath)\n",
    "\n",
    "# the scene mesh\n",
    "scan_path = root / f'{scan_name}/{scan_name}_voxelized.ply'\n",
    "print(scan_path)\n",
    "scan = trimesh.load(scan_path)\n",
    "scan.vertices *= voxel_size\n",
    "print(scan)\n",
    "print('Add scan')\n",
    "scene.add_geometry(scan)\n",
    "\n",
    "# draw the chunk\n",
    "# required transform is wrt the center of the box, which is what we have\n",
    "# hence use the existing g2w\n",
    "box = trimesh.creation.box(subvol_size, g2w)\n",
    "print('Add box')\n",
    "scene.add_geometry(box)\n",
    "\n",
    "# axes\n",
    "axes = trimesh.creation.axis(axis_radius=0.1, axis_length=10)\n",
    "print('Add axes')\n",
    "scene.add_geometry(axes)\n",
    "\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25851c31-83f0-437d-b157-9596b3abd79d",
   "metadata": {},
   "source": [
    "## Vis only occupied voxels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94f72d-7f08-4adf-8f8e-aea233b1bf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scene = trimesh.scene.scene.Scene() \n",
    "\n",
    "scan_name = get_scan_name(sceneid, scanid)\n",
    "print(scan_name)\n",
    "root = Path('/mnt/data/scannet/scans')\n",
    "scan_path = root / f'{scan_name}/{scan_name}_voxelized.ply'\n",
    "print(scan_path)\n",
    "scan = trimesh.load(scan_path)\n",
    "scan.vertices *= voxel_size\n",
    "print(scan)\n",
    "print('Add scan')\n",
    "scene.add_geometry(scan)\n",
    "\n",
    "# get the actual subvol\n",
    "x = f['x'][ndx]\n",
    "voxel_dims = (1, 1, 1)\n",
    "\n",
    "print('Draw occupied voxels')\n",
    "for i in tqdm(range(subvol_size[0])):\n",
    "    for j in range(subvol_size[1]):\n",
    "        for k in range(subvol_size[2]):\n",
    "            if x[i, j, k] == 1:\n",
    "                # get the transformation of this voxel\n",
    "                # w2g is wrt center of the chunk, but ijk is wrt a corner of the chunk\n",
    "                # hence subtract half chunk size from ijk to get \"grid coord\"\n",
    "                t = torch.eye(4)\n",
    "                t[:3, -1] = -(torch.Tensor((i, j, k)) - 16)\n",
    "                \n",
    "                voxel_w2g = t @ w2g\n",
    "                voxel_g2w = torch.inverse(torch.Tensor(voxel_w2g)).numpy()\n",
    "                box = trimesh.creation.box(voxel_dims, voxel_g2w)\n",
    "                # make the box blue\n",
    "                box.visual.face_colors = np.zeros((12, 4)) + (0, 0, 255, 128)\n",
    "                box.visual.vertex_colors = np.zeros((8, 4)) + (0, 0, 255, 255)\n",
    "                scene.add_geometry(box)\n",
    "\n",
    "intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], (40, 30))\n",
    "\n",
    "pose_path = root / scan_name / 'pose' / f'{frames[0]}.txt'\n",
    "pose = load_pose(pose_path).numpy()\n",
    "focal = (intrinsic[0, 0], intrinsic[1, 1])\n",
    "cam = trimesh.scene.Camera(name=f'{scan_name}_{frames[0]}', resolution=(40, 30), focal=focal, z_near=0.4, z_far=4.0) \n",
    "camball, campath = trimesh.creation.camera_marker(cam, marker_height=4, origin_size=0.05)\n",
    "camball.apply_transform(pose)\n",
    "campath.apply_transform(pose)\n",
    "campath.colors = np.ones((5, 3)) * np.array((0, 255, 0))\n",
    "print('Add cam')\n",
    "scene.add_geometry(camball)\n",
    "scene.add_geometry(campath)\n",
    "\n",
    "axes = trimesh.creation.axis(axis_radius=0.1, axis_length=10)\n",
    "print('Add axes')\n",
    "scene.add_geometry(axes)\n",
    "\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d022468e-3922-42e9-a8e4-bb849da1b0a8",
   "metadata": {},
   "source": [
    "## project image to voxels and viz correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f35bd8-b944-45d5-9187-49fabcffc0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_img_size = (160, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69fd714-9669-49df-938d-f1d860aa4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_path = root / scan_name / 'pose' / f'{frames[0]}.txt'\n",
    "pose = load_pose(pose_path).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d511e771-363c-40d3-8186-f16798efe8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_path = root / scan_name / 'depth' / f'{frames[0]}.png' \n",
    "# invert dims in the tensor\n",
    "# N, H, W -> torch nn convention\n",
    "depth_big = load_depth(depth_path, (640, 480))\n",
    "print(depth_big.shape)\n",
    "plt.imshow(depth_big)\n",
    "plt.axis('off')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40246cc-668a-4ca9-85d6-53ac06b03299",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_big.min(), depth_big.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca531e0d-bee2-4c11-b4ae-a7ff8ff01728",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_path = root / scan_name / 'depth' / f'{frames[0]}.png' \n",
    "# invert dims in the tensor\n",
    "# N, H, W -> torch nn convention\n",
    "depth = load_depth(depth_path, proj_img_size)\n",
    "print(depth.shape)\n",
    "plt.axis('off')\n",
    "plt.imshow(depth)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b779e09c-c06e-4c58-aa67-ed03ff75b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth.min(), depth.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06261b-af27-4328-8461-b1e40ce232f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# original RGB image\n",
    "rgb_path = root / scan_name / 'color' / f'{frames[0]}.jpg' \n",
    "rgb = load_color(rgb_path, (320, 240))\n",
    "print(rgb.shape)\n",
    "plt.axis('off')\n",
    "plt.imshow(rgb.transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0048c-4de4-45d4-be0e-097202831d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small RGB image\n",
    "rgb_path = root / scan_name / 'color' / f'{frames[0]}.jpg' \n",
    "rgb = load_color(rgb_path, proj_img_size)\n",
    "print(rgb.shape)\n",
    "plt.axis('off')\n",
    "plt.imshow(rgb.transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d26b7-6d0d-4e61-8fd1-438d3e687f0b",
   "metadata": {},
   "source": [
    "## Draw correspondences between image/depth and chunk voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a73a22-43f7-488a-80ea-5bcd4febfc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get projection\n",
    "intrinsic_path = root / scan_name / 'intrinsic/intrinsic_color.txt'\n",
    "intrinsic = load_intrinsic(intrinsic_path)\n",
    "# adjust for smaller image size\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], proj_img_size)\n",
    "\n",
    "projection = ProjectionHelper(\n",
    "            intrinsic, \n",
    "            0.4, 4.0,\n",
    "            proj_img_size,\n",
    "            subvol_size, voxel_size\n",
    "        )\n",
    "\n",
    "# projection expects origin of chunk in a corner\n",
    "# but w2g is wrt center of the chunk -> add 16 to its \"grid coords\" to get the required grid coords\n",
    "# ie 0,0,0 becomes 16,16,16\n",
    "# add an additional translation to existing one \n",
    "t = torch.eye(4)\n",
    "t[:3, -1] = 16\n",
    "w2g_tmp = t @ w2g\n",
    "\n",
    "proj3d, proj2d = projection.compute_projection(torch.Tensor(depth), torch.Tensor(pose), torch.Tensor(w2g_tmp))\n",
    "print(proj3d.shape, proj2d.shape)\n",
    "num_inds = proj3d[0]\n",
    "print('Num correspondences:', proj3d[0], proj2d[0])\n",
    "ind3d = proj3d[1:1+num_inds]\n",
    "ind2d = proj2d[1:1+num_inds]\n",
    "print(ind3d.min(), ind3d.max(), torch.unique(ind3d).shape, torch.prod(torch.Tensor(subvol_size)))\n",
    "print(ind2d.min(), ind2d.max(), torch.unique(ind2d).shape, torch.prod(torch.Tensor(proj_img_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c39d08-8f92-4486-9a48-431ee1e381ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of occupied voxels\n",
    "x = f['x'][ndx]\n",
    "print('Occupied voxels', (x == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831cd01b-c206-4a5a-ba58-c7c7c04b2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ijk coordinates into the chunk (array indices)\n",
    "coords_3d = torch.empty(4, num_inds)\n",
    "coords_3d = ProjectionHelper.lin_ind_to_coords_static(ind3d, coords_3d, subvol_size).T[:, :-1].long()\n",
    "print('3d coords:', coords_3d.shape, coords_3d.dtype)\n",
    "# viz in red\n",
    "colors = torch.zeros(num_inds, 3, dtype=int)\n",
    "colors[:, 0] = (torch.arange(num_inds) * 255 / num_inds).floor()\n",
    "print('colors:', colors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5262c0-7836-48e7-9d2a-dfc56f28dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_3d.min(axis=0)[0], coords_3d.max(axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a67107-508b-4bf9-8084-3f09aaf7cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_2d = torch.empty(2, num_inds, dtype=torch.long)\n",
    "coords_2d = ProjectionHelper.lin_ind2d_to_coords2d_static(ind2d, coords_2d, proj_img_size).T.numpy()\n",
    "\n",
    "color_2d = np.zeros(proj_img_size[::-1] + (3,), dtype=int)\n",
    "color_2d[coords_2d[:, 1], coords_2d[:, 0]] = colors\n",
    "plt.imshow(color_2d)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50979f-841c-4251-a450-00a4e6bed011",
   "metadata": {},
   "source": [
    "## draw occupied and mapped voxels with colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039136ec-7d9a-4ed4-a713-e5942361e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = trimesh.scene.scene.Scene() \n",
    "\n",
    "scan_name = get_scan_name(sceneid, scanid)\n",
    "print(scan_name)\n",
    "root = Path('/mnt/data/scannet/scans')\n",
    "scan_path = root / f'{scan_name}/{scan_name}_vh_clean_2.ply'\n",
    "print(scan_path)\n",
    "scan = trimesh.load(scan_path)\n",
    "print(scan)\n",
    "print('Add scan')\n",
    "scene.add_geometry(scan)\n",
    "\n",
    "# get the actual subvol\n",
    "x = f['x'][ndx]\n",
    "dims = x.shape\n",
    "voxel_dims = (1, 1, 1)\n",
    "\n",
    "print('Draw occupied voxels')\n",
    "for i in tqdm(range(dims[0])):\n",
    "    for j in range(dims[1]):\n",
    "        for k in range(dims[2]):\n",
    "            if x[i, j, k] == 1:\n",
    "                # get the transformation of this voxel\n",
    "                # w2g is wrt center of the chunk, but ijk is wrt a corner of the chunk\n",
    "                # hence subtract half chunk size from ijk to get \"grid coord\"\n",
    "                t = torch.eye(4)\n",
    "                t[:3, -1] = -(torch.Tensor((i, j, k)) - 16)\n",
    "                # add an additional translation to existing one\n",
    "                voxel_w2g = t @ w2g\n",
    "                voxel_g2w = torch.inverse(torch.Tensor(voxel_w2g)).numpy()\n",
    "                box = trimesh.creation.box(voxel_dims, voxel_g2w)\n",
    "                # make the box blue\n",
    "                box.visual.face_colors = np.zeros((12, 4)) + (0, 0, 255, 128)\n",
    "                box.visual.vertex_colors = np.zeros((8, 4)) + (0, 0, 255, 255)\n",
    "                scene.add_geometry(box)\n",
    "\n",
    "intersection = 0                \n",
    "print('Draw mapped voxels')\n",
    "for coord_3d, color in tqdm(zip(coords_3d, colors)): \n",
    "    i, j, k = coord_3d.tolist()\n",
    "    if x[i, j, k] == 1:\n",
    "        intersection += 1\n",
    "    # get the transformation of this voxel\n",
    "    # w2g is wrt center of the chunk, but ijk is wrt a corner of the chunk\n",
    "    # hence subtract half chunk size from ijk to get \"grid coord\"\n",
    "    t = torch.eye(4)\n",
    "    t[:3, -1] = -(torch.Tensor((i, j, k)) - 16)\n",
    "    # add an additional translation to existing one\n",
    "    voxel_w2g = t @ w2g\n",
    "    voxel_g2w = torch.inverse(torch.Tensor(voxel_w2g)).numpy()\n",
    "    box = trimesh.creation.box(voxel_dims, voxel_g2w)\n",
    "\n",
    "    color_tup = tuple(color.tolist())\n",
    "    box.visual.face_colors = np.zeros((12, 4)) + (color_tup + (128,))\n",
    "    box.visual.vertex_colors = np.zeros((8, 4)) + (color_tup + (255,))\n",
    "    scene.add_geometry(box)\n",
    "\n",
    "print(f'Mapped voxels: {len(coords_3d)}')\n",
    "print(f'Occupied voxels: {(x == 1).sum()}')\n",
    "print(f'Mapped and occupied voxels: {intersection} ')\n",
    "\n",
    "intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], (40, 30))\n",
    "\n",
    "pose_path = root / scan_name / 'pose' / f'{frames[0]}.txt'\n",
    "pose = load_pose(pose_path).numpy()\n",
    "focal = (intrinsic[0, 0], intrinsic[1, 1])\n",
    "cam = trimesh.scene.Camera(name=f'{scan_name}_{frames[0]}', resolution=(40, 30), focal=focal, z_near=0.4, z_far=4.0) \n",
    "camball, campath = trimesh.creation.camera_marker(cam, marker_height=4, origin_size=0.05)\n",
    "camball.apply_transform(pose)\n",
    "campath.apply_transform(pose)\n",
    "campath.colors = np.ones((5, 3)) * np.array((0, 255, 0))\n",
    "print('Add cam')\n",
    "scene.add_geometry(camball)\n",
    "scene.add_geometry(campath)\n",
    "\n",
    "axes = trimesh.creation.axis(axis_radius=0.1, axis_length=10)\n",
    "print('Add axes')\n",
    "scene.add_geometry(axes)\n",
    "\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f462f-1877-4d2d-b2de-c148f3aed260",
   "metadata": {},
   "source": [
    "## Viz correspondence b/w mapped voxels and pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d3a65-caed-4041-a20f-578584adf6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_pix(grid_ndx, w2g, cam2world, intrinsic, subvol_size):\n",
    "    '''\n",
    "    map grid(chunk) indices to pixel coordinates\n",
    "    '''\n",
    "    # convert ijk to grid coords, then grid to world and world to cam\n",
    "    grid_coords = grid_ndx - torch.Tensor(subvol_size)/2\n",
    "    # convert to homogenous to use transformation matrix\n",
    "    grid_coords = torch.cat([grid_coords, torch.ones(len(grid_coords), 1)], dim=-1)\n",
    "    # apply g2w\n",
    "    world_coords = torch.Tensor(g2w) @ grid_coords.T\n",
    "    \n",
    "    world2cam = torch.inverse(cam2world)\n",
    "    cam_coords = world2cam @ world_coords\n",
    "    \n",
    "    p = cam_coords\n",
    "    \n",
    "    x2d = (p[0] * intrinsic[0][0]) / p[2] + intrinsic[0][2]\n",
    "    y2d = (p[1] * intrinsic[1][1]) / p[2] + intrinsic[1][2]\n",
    "\n",
    "    pixel_coords = torch.vstack([x2d, y2d]).T.floor().long().numpy()\n",
    "    \n",
    "    return pixel_coords    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32f6ab-fbcc-4751-aa0e-b25393a681fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first map the projected voxels to pixel coords and use a single color, should match the previous image\n",
    "\n",
    "print('Projected voxels:', coords_3d.shape)\n",
    "\n",
    "x = f['x'][ndx]\n",
    "dims = x.shape\n",
    "occ_coords = []\n",
    "for i in range(dims[0]):\n",
    "    for j in range(dims[1]):\n",
    "        for k in range(dims[2]):\n",
    "            if x[i,j,k] == 1:\n",
    "                occ_coords.append((i,j,k))\n",
    "occ_coords = torch.Tensor(occ_coords)\n",
    "print(f'Occupied voxels:', len(occ_coords))\n",
    "\n",
    "# read pose\n",
    "pose_path = root / scan_name / 'pose' / f'{frames[0]}.txt'\n",
    "cam2world = load_pose(pose_path)\n",
    "# read intrinsic\n",
    "intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], proj_img_size)\n",
    "\n",
    "proj_pixelcoords = grid_to_pix(coords_3d, w2g, cam2world, intrinsic, subvol_size)\n",
    "print('Num proj pixels', proj_pixelcoords.shape)\n",
    "print('Proj Pixel coords range', proj_pixelcoords.min(axis=0), proj_pixelcoords.max(axis=0))\n",
    "\n",
    "occ_pixelcoords = grid_to_pix(occ_coords, w2g, cam2world, intrinsic, subvol_size)\n",
    "valid = (occ_pixelcoords[:, 0] >= 0) \\\n",
    "        * (occ_pixelcoords[:, 1] >= 0) \\\n",
    "        * (occ_pixelcoords[:, 0] < proj_img_size[0]) \\\n",
    "        * (occ_pixelcoords[:, 1] < proj_img_size[1])\n",
    "occ_pixelcoords = occ_pixelcoords[valid]\n",
    "print('Num occ pixels', occ_pixelcoords.shape)\n",
    "print('Occ Pixel coords range', occ_pixelcoords.min(axis=0), occ_pixelcoords.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a5f8b-4c4a-43b6-9d95-4ce73cb7bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the mapped voxels in 2d\n",
    "vox_2d = np.zeros(proj_img_size[::-1] + (3,), dtype=np.uint8)\n",
    "# Y,X / H,W indexing\n",
    "vox_2d[proj_pixelcoords[:, 1], proj_pixelcoords[:, 0]] = colors\n",
    "plt.imshow(vox_2d)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5c165-4490-49ca-a115-2e2f7052f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the occupied voxels in 2d\n",
    "occ_2d = np.zeros(proj_img_size[::-1] + (3,), dtype=np.uint8)\n",
    "# Y,X / H,W indexing\n",
    "occ_2d[occ_pixelcoords[:, 1], occ_pixelcoords[:, 0]] = np.array((0, 0, 255), dtype=int)\n",
    "plt.imshow(occ_2d)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18999b05-34f2-4f57-8c77-bb80110fbc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz both in 1 image\n",
    "both_2d = np.zeros(proj_img_size[::-1] + (3,), dtype=np.uint8)\n",
    "# Y,X / H,W indexing\n",
    "both_2d[occ_pixelcoords[:, 1], occ_pixelcoords[:, 0]] = np.array((0, 0, 255), dtype=int)\n",
    "both_2d[proj_pixelcoords[:, 1], proj_pixelcoords[:, 0]] = colors\n",
    "plt.imshow(both_2d)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7917193d-5e94-45bc-8f50-f76cb91e0a93",
   "metadata": {},
   "source": [
    "## Draw point cloud from the camera pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df6344-b3be-46b3-8270-6395b2160999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def world_to_pix(world_coords, cam2world, intrinsic, subvol_size, img_size):\n",
    "    '''\n",
    "    map grid(chunk) indices to pixel coordinates\n",
    "    img_size: W, H\n",
    "    '''\n",
    "    world_coords = torch.cat([world_coords, torch.ones(len(world_coords), 1)], dim=-1).T\n",
    "    world2cam = torch.inverse(cam2world)\n",
    "    cam_coords = world2cam @ world_coords\n",
    "    \n",
    "    p = cam_coords\n",
    "    \n",
    "    x2d = (p[0] * intrinsic[0][0]) / p[2] + intrinsic[0][2]\n",
    "    y2d = (p[1] * intrinsic[1][1]) / p[2] + intrinsic[1][2]\n",
    "\n",
    "    # coords within image\n",
    "    valid_pix = (x2d >= 0) \\\n",
    "                * (y2d >= 0) \\\n",
    "                * (x2d <= img_size[0]) \\\n",
    "                * (y2d <= img_size[1])\n",
    "    # pick only positive depth\n",
    "    depth_mask = p[2] > 0\n",
    "    \n",
    "    mask = depth_mask * valid_pix\n",
    "    \n",
    "    x2d, y2d = x2d[mask], y2d[mask]\n",
    "    pixel_coords = torch.vstack([x2d, y2d]).T.floor().long().numpy()\n",
    "    \n",
    "    return pixel_coords, mask   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b12fe-ca8a-4f39-942f-3170a864f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_name = get_scan_name(sceneid, scanid)\n",
    "print(scan_name)\n",
    "root = Path('/mnt/data/scannet/scans')\n",
    "scan_path = root / f'{scan_name}/{scan_name}_vh_clean_2.ply'\n",
    "print(scan_path)\n",
    "scan = trimesh.load(scan_path)\n",
    "print('Vertices', scan.vertices.shape, scan.visual.vertex_colors.shape)\n",
    "\n",
    "# read pose\n",
    "pose_path = root / scan_name / 'pose' / f'{frames[0]}.txt'\n",
    "cam2world = load_pose(pose_path)\n",
    "# read intrinsic\n",
    "intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], proj_img_size)\n",
    "\n",
    "print(scan.vertices.min(axis=0), scan.vertices.max(axis=0))\n",
    "\n",
    "pc_pixelcoords, mask = world_to_pix(torch.Tensor(scan.vertices), cam2world, intrinsic, subvol_size, proj_img_size)\n",
    "print('Num PC pixels', pc_pixelcoords.shape)\n",
    "print('PC Pixel coords range', pc_pixelcoords.min(axis=0), pc_pixelcoords.max(axis=0))\n",
    "pc_colors = scan.visual.vertex_colors[mask][:, :3]\n",
    "print('New colors', pc_colors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e6d0e-b395-4ed7-abb4-d99e93a1dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the point cloud from the chosen pose in 2d\n",
    "pc_2d = np.ones(proj_img_size[::-1] + (3,), dtype=np.uint8) * 255\n",
    "# Y,X / H,W indexing\n",
    "pc_2d[pc_pixelcoords[:, 1], pc_pixelcoords[:, 0]] = pc_colors\n",
    "plt.imshow(pc_2d)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706bae7-0a80-4cca-b7d3-894bb9a7f176",
   "metadata": {},
   "source": [
    "## Draw matches between RGB and rendered PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15c050-9a6d-4ea5-bf55-8e6913b37c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.axis('off')\n",
    "# select a few correspondences to draw\n",
    "n_matches = 12\n",
    "match_inds = torch.randperm(len(proj_pixelcoords))[:n_matches]\n",
    "match_coords = proj_pixelcoords[match_inds]\n",
    "# interchange X and Y\n",
    "match_coords = np.vstack([match_coords[:, 1], match_coords[:, 0]]).T\n",
    "matches = torch.arange(n_matches).repeat(2, 1).T\n",
    "\n",
    "plot_matches(ax, np.transpose(rgb, (1, 2, 0)), pc_2d, match_coords, match_coords.copy(), matches,\n",
    "             only_matches=False, keypoints_color='white', matches_color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4583a39-032f-4dab-91f3-1a7b6c735e73",
   "metadata": {},
   "source": [
    "## Draw all the cameras in a scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508b34f-b9ad-468f-8116-be49a323b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw all cameras in a scene\n",
    "scene = trimesh.scene.scene.Scene() \n",
    "\n",
    "scan_name = get_scan_name(sceneid, scanid)\n",
    "print(scan_name)\n",
    "root = Path('/mnt/data/scannet/scans')\n",
    "scan_path = root / f'{scan_name}/{scan_name}_vh_clean_2.ply'\n",
    "scan = trimesh.load(scan_path)\n",
    "print(scan)\n",
    "print('Add scan')\n",
    "scene.add_geometry(scan)\n",
    "\n",
    "box = trimesh.creation.box(subvol_size, g2w)\n",
    "print('Add box')\n",
    "scene.add_geometry(box)\n",
    "\n",
    "intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], (40, 30))\n",
    "pose_files = sorted(os.listdir(root / scan_name / 'pose'), key=lambda f: int(osp.splitext(f)[0]))\n",
    "pose_indices = range(0, len(pose_files), frame_skip)\n",
    "print('Add poses:', len(pose_indices))\n",
    "frame_skip = 40\n",
    "focal = (intrinsic[0, 0], intrinsic[1, 1]) \n",
    "for ndx, pose_ndx in enumerate(tqdm(pose_indices)):\n",
    "    pose_path = root / scan_name / 'pose' / pose_files[pose_ndx]\n",
    "    pose = load_pose(pose_path).numpy()\n",
    "    \n",
    "    cam = trimesh.scene.Camera(name=f'{scan_name}_{frames[0]}', resolution=(40, 30), focal=focal, z_near=0.4, z_far=4.0) \n",
    "    camball, campath = trimesh.creation.camera_marker(cam, marker_height=4, origin_size=0.05)\n",
    "    \n",
    "    camball.apply_transform(pose)\n",
    "    campath.apply_transform(pose)\n",
    "    campath.colors = np.zeros((5, 3))\n",
    "    scene.add_geometry(camball)\n",
    "    scene.add_geometry(campath)\n",
    "\n",
    "# draw the best pose in a different color\n",
    "pose_path = root / scan_name / 'pose' / f'{frames[0]}.txt'\n",
    "pose = load_pose(pose_path).numpy()\n",
    "focal = (intrinsic[0, 0], intrinsic[1, 1])\n",
    "cam = trimesh.scene.Camera(name=f'{scan_name}_{frames[0]}', resolution=(40, 30), focal=focal, z_near=0.4, z_far=4.0) \n",
    "camball, campath = trimesh.creation.camera_marker(cam, marker_height=4, origin_size=0.05)\n",
    "camball.apply_transform(pose)\n",
    "campath.apply_transform(pose)\n",
    "# green\n",
    "campath.colors = np.ones((5, 3)) * np.array((0, 255, 0))\n",
    "print('Add cam')\n",
    "scene.add_geometry(camball)\n",
    "scene.add_geometry(campath)\n",
    "\n",
    "axes = trimesh.creation.axis(axis_radius=0.1, axis_length=10)\n",
    "print('Add axes')\n",
    "scene.add_geometry(axes)\n",
    "\n",
    "scene.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
