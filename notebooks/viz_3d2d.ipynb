{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c52d06-8487-4a80-b3be-56b77baf18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198dc03e-1bb9-4a00-871e-1e5807b6cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "\n",
    "from pathlib import Path\n",
    "import trimesh\n",
    "import pyrender\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import os, os.path as osp\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import plot_matches\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from datasets.scannet.utils_3d import ProjectionHelper, adjust_intrinsic, make_intrinsic, load_intrinsic, load_pose\n",
    "from datasets.scannet.utils_3d import load_depth, load_color\n",
    "from scripts.sem_seg.prep_backproj_data import get_world_to_scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3294419-1a37-4905-a6c8-e055984bbdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scan_name(scene_id, scan_id):\n",
    "    return f'scene{str(scene_id).zfill(4)}_{str(scan_id).zfill(2)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80458c51-6694-4050-b364-dca72631c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# globals\n",
    "subvol_size = (32, 32, 64)\n",
    "voxel_size = 0.05\n",
    "voxel_dims = (1, 1, 1)\n",
    "root = Path('/mnt/data/scannet/scans')\n",
    "proj_img_size = (40, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516e802-57e6-4f88-a396-1376265e06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/mnt/data/scannet/backproj')\n",
    "fname = 'tmp.h5'\n",
    "f = h5py.File(data_dir / fname, 'r')\n",
    "print(f.keys())\n",
    "print(f['x'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d558ca-f433-43ae-8e6f-b295937fe834",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115cbdb-ae72-48bf-a12f-2c98366389f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one sample\n",
    "ndx = 99\n",
    "w2g, sceneid, scanid, frames = f['world_to_grid'][ndx], f['scene_id'][ndx], f['scan_id'][ndx], f['frames'][ndx]\n",
    "print(w2g)\n",
    "print(sceneid, scanid)\n",
    "print(frames)\n",
    "g2w = torch.inverse(torch.Tensor(w2g)).numpy()\n",
    "\n",
    "subvol_x = f['x'][ndx]\n",
    "print(f'xmin: {subvol_x.min()}, xmax: {subvol_x.max()}, occupied: {(subvol_x == 1).sum()}')\n",
    "# per-scene basics\n",
    "scan_name = get_scan_name(sceneid, scanid)\n",
    "print(scan_name)\n",
    "scan_path = root / f'{scan_name}/{scan_name}_vh_clean_2.ply'\n",
    "scan = trimesh.load(scan_path)\n",
    "\n",
    "# camera and frustum\n",
    "intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], (40, 30))\n",
    "focal = (intrinsic[0, 0], intrinsic[1, 1])\n",
    "scan_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b85ad9-4c89-40fc-bf04-f97aa2a250f9",
   "metadata": {},
   "source": [
    "## Draw one chunk and its corresponding camera pose(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926a3d2-8f6e-4fdf-b6eb-c9acca44ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = trimesh.scene.scene.Scene() \n",
    "\n",
    "for cam_ndx in range(5):\n",
    "    pose_path = root / scan_name / 'pose' / f'{frames[cam_ndx]}.txt'\n",
    "    pose = load_pose(pose_path).numpy()\n",
    "    cam = trimesh.scene.Camera(name=f'{scan_name}_{frames[cam_ndx]}', resolution=(40, 30), focal=focal, z_near=0.4, z_far=4.0) \n",
    "\n",
    "    camball, campath = trimesh.creation.camera_marker(cam, marker_height=4, origin_size=0.05)\n",
    "    camball.apply_transform(pose)\n",
    "    campath.apply_transform(pose)\n",
    "    color = (255, 0, 0) if cam_ndx == 0 else (0, 255, 0)\n",
    "    campath.colors = np.ones((5, 3)) * np.array(color)\n",
    "    scene.add_geometry(camball)\n",
    "    scene.add_geometry(campath)\n",
    "\n",
    "# the scene mesh\n",
    "scene.add_geometry(scan)\n",
    "\n",
    "# draw the chunk\n",
    "# get transform wrt center of box\n",
    "t = torch.eye(4)\n",
    "t[:3, -1] = -torch.Tensor(subvol_size)/2\n",
    "center_w2g = t @ w2g\n",
    "center_g2w = torch.inverse(center_w2g).numpy()\n",
    "box = trimesh.creation.box(subvol_size, center_g2w)\n",
    "print('Add chunk')\n",
    "scene.add_geometry(box)\n",
    "\n",
    "# axes\n",
    "axes = trimesh.creation.axis(axis_radius=0.1, axis_length=10)\n",
    "print('Add axes')\n",
    "scene.add_geometry(axes)\n",
    "\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25851c31-83f0-437d-b157-9596b3abd79d",
   "metadata": {},
   "source": [
    "## Vis only occupied voxels on mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94f72d-7f08-4adf-8f8e-aea233b1bf97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scene = trimesh.scene.scene.Scene() \n",
    "\n",
    "scene.add_geometry(scan)\n",
    "\n",
    "print('Draw occupied voxels')\n",
    "for i in tqdm(range(subvol_size[0])):\n",
    "    for j in range(subvol_size[1]):\n",
    "        for k in range(subvol_size[2]):\n",
    "            if subvol_x[i, j, k] == 1:\n",
    "                # get the transformation of this voxel\n",
    "                t = torch.eye(4)\n",
    "                t[:3, -1] = -torch.Tensor((i, j, k))\n",
    "                \n",
    "                voxel_w2g = t @ w2g\n",
    "                voxel_g2w = torch.inverse(torch.Tensor(voxel_w2g)).numpy()\n",
    "                box = trimesh.creation.box(voxel_dims, voxel_g2w)\n",
    "                # make the box blue\n",
    "                box.visual.face_colors = np.zeros((12, 4)) + (0, 0, 255, 128)\n",
    "                box.visual.vertex_colors = np.zeros((8, 4)) + (255, 0, 0, 255)\n",
    "                scene.add_geometry(box)\n",
    "\n",
    "scene.add_geometry(scan)\n",
    "\n",
    "# axes\n",
    "axes = trimesh.creation.axis(axis_radius=0.1, axis_length=10)\n",
    "scene.add_geometry(axes)\n",
    "\n",
    "pose_path = root / scan_name / 'pose' / f'{frames[0]}.txt'\n",
    "pose = load_pose(pose_path).numpy()\n",
    "cam = trimesh.scene.Camera(name=f'{scan_name}_{frames[0]}', resolution=(40, 30), focal=focal, z_near=0.4, z_far=4.0) \n",
    "camball, campath = trimesh.creation.camera_marker(cam, marker_height=4, origin_size=0.05)\n",
    "camball.apply_transform(pose)\n",
    "campath.apply_transform(pose)\n",
    "campath.colors = np.ones((5, 3)) * np.array((0, 255, 0))\n",
    "scene.add_geometry(camball)\n",
    "scene.add_geometry(campath)\n",
    "\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34639e-c1b1-43ac-a366-495f7b7e370e",
   "metadata": {},
   "source": [
    "## Viz voxelized scene and occupied voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d4d1f2-c165-492f-815c-5192dd43a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = trimesh.scene.scene.Scene() \n",
    "\n",
    "data = torch.load(root / scan_name / f'{scan_name}_occ_grid.pth')\n",
    "scene_x = data['x']\n",
    "start_ndx = data['start_ndx']\n",
    "\n",
    "# world to scene\n",
    "w2s = get_world_to_scene(voxel_size, -start_ndx)\n",
    "size = scene_x.shape\n",
    "\n",
    "for i in tqdm(range(size[0])):\n",
    "    for j in range(size[1]):\n",
    "        for k in range(size[2]):\n",
    "            if scene_x[i, j, k] == 1:\n",
    "                t = torch.eye(4)\n",
    "                t[:3, -1] = -(torch.Tensor((i, j, k)))\n",
    "                \n",
    "                voxel_w2s = t @ w2s\n",
    "                voxel_s2w = torch.inverse(torch.Tensor(voxel_w2s)).numpy()\n",
    "                box = trimesh.creation.box(voxel_dims, voxel_s2w)\n",
    "                box.visual.face_colors = np.zeros((12, 4)) + (128, 128, 128, 192)\n",
    "                box.visual.vertex_colors = np.zeros((8, 4)) + (128, 128, 128, 192)\n",
    "                scene.add_geometry(box)\n",
    "\n",
    "for i in tqdm(range(subvol_size[0])):\n",
    "    for j in range(subvol_size[1]):\n",
    "        for k in range(subvol_size[2]):\n",
    "            if subvol_x[i, j, k] == 1:\n",
    "                # get the transformation of this voxel\n",
    "                # w2g is wrt center of the chunk, but ijk is wrt a corner of the chunk\n",
    "                # hence subtract half chunk size from ijk to get \"grid coord\"\n",
    "                t = torch.eye(4)\n",
    "                t[:3, -1] = -torch.Tensor((i, j, k))\n",
    "                \n",
    "                voxel_w2g = t @ w2g\n",
    "                voxel_g2w = torch.inverse(torch.Tensor(voxel_w2g)).numpy()\n",
    "                box = trimesh.creation.box(voxel_dims, voxel_g2w)\n",
    "                # make the box blue\n",
    "                box.visual.face_colors = np.zeros((12, 4)) + (0, 0, 255, 192)\n",
    "                box.visual.vertex_colors = np.zeros((8, 4)) + (255, 0, 0, 192)\n",
    "                scene.add_geometry(box)\n",
    "\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb658270-89ec-4a7e-be49-3242eb3ef211",
   "metadata": {},
   "source": [
    "## Look at the scene from the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43be802-f71f-4f84-b3b9-fae35620950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed2e61-1fc1-4ec2-8cf3-289cd58d808b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "render_intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "render_intrinsic = adjust_intrinsic(render_intrinsic, [1296, 968], proj_img_size)\n",
    "render_focal = (render_intrinsic[0, 0], render_intrinsic[1, 1])\n",
    "cx, cy = render_intrinsic[0, 2], render_intrinsic[1, 2]\n",
    "rotx = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, -1, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [ 0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "# rotx\n",
    "pose_fixed =  pose @ rotx @ rotx\n",
    "render_cam = pyrender.IntrinsicsCamera(render_focal[0], render_focal[1], cx, cy, znear=0.4, zfar=4.0, name=None)\n",
    "render_light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=2.0)\n",
    "r = pyrender.OffscreenRenderer(viewport_width=proj_img_size[0],\n",
    "                                viewport_height=proj_img_size[1],\n",
    "                                point_size=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e617094-8a67-4715-9b9f-42a4944bab0c",
   "metadata": {},
   "source": [
    "### Voxelized Mesh + Occupied Voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde253f6-0235-421e-87df-64a66dad5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scene with transparent chunk\n",
    "chunkscene = trimesh.scene.scene.Scene() \n",
    "box = trimesh.creation.box(subvol_size, g2w)\n",
    "box.visual.face_colors = np.zeros((12, 4)) + (255, 0, 0, 192)\n",
    "box.visual.vertex_colors = np.zeros((8, 4)) + (255, 0, 0, 192)\n",
    "chunkscene.add_geometry(box)\n",
    "chunkmesh = chunkscene.dump(concatenate=True)\n",
    "chunkmesh = pyrender.Mesh.from_trimesh(chunkmesh, smooth=False, wireframe=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca595a-8411-4b58-b7e9-78fcacdfd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "voxmesh = scene.dump(concatenate=True)\n",
    "print(voxmesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d09ce72-e17f-4ea7-a378-8baa7badb1a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyscene = pyrender.Scene()\n",
    "pymesh = pyrender.Mesh.from_trimesh(voxmesh, smooth=False)\n",
    "\n",
    "pyscene.add(pymesh)\n",
    "pyscene.add(chunkmesh)\n",
    "pyscene.add(render_cam, pose=pose_fixed)\n",
    "pyscene.add(render_light, pose=pose_fixed)\n",
    "\n",
    "rgb_from_pose_vox, _ = r.render(pyscene)\n",
    "print(rgb_from_pose_vox.shape)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 9)\n",
    "plt.imshow(rgb_from_pose_vox)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da4e1fe-8b8e-4ee2-8a43-89874726fb78",
   "metadata": {},
   "source": [
    "### Original Mesh + Occupied Voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77094297-b26c-4b22-9c35-660d3024470c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyscene = pyrender.Scene()\n",
    "pymesh = pyrender.Mesh.from_trimesh(scan)\n",
    "\n",
    "pyscene.add(pymesh)\n",
    "pyscene.add(render_cam, pose=pose_fixed)\n",
    "pyscene.add(render_light, pose=pose_fixed)\n",
    "\n",
    "rgb_from_pose, _ = r.render(pyscene)\n",
    "print(rgb_from_pose.shape)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 9)\n",
    "plt.imshow(rgb_from_pose)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9922e-4188-40c3-807a-2e16b0e973b7",
   "metadata": {},
   "source": [
    "### Overlay transparent voxels on rendered RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c4a21-367d-4c91-abb3-5b9358dcd452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bg = Image.fromarray(rgb_from_pose.astype('uint8'), 'RGB').convert(\"RGBA\")\n",
    "fg = Image.fromarray(rgb_from_pose_vox.astype('uint8'), 'RGB').convert(\"RGBA\")\n",
    "overlay_img = np.asarray(Image.blend(bg, fg, 0.6).convert('RGB'))\n",
    "print(overlay_img.shape)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 9)\n",
    "plt.imshow(overlay_img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d022468e-3922-42e9-a8e4-bb849da1b0a8",
   "metadata": {},
   "source": [
    "## project image to voxels and viz correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae5441-4a61-4aeb-93f5-f998d9dfeda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_ndx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69fd714-9669-49df-938d-f1d860aa4290",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_path = root / scan_name / 'pose' / f'{frames[frame_ndx]}.txt'\n",
    "pose = load_pose(pose_path).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d511e771-363c-40d3-8186-f16798efe8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_path = root / scan_name / 'depth' / f'{frames[frame_ndx]}.png' \n",
    "# invert dims in the tensor\n",
    "# N, H, W -> torch nn convention\n",
    "depth_big = load_depth(depth_path, (640, 480))\n",
    "print(depth_big.shape)\n",
    "plt.imshow(depth_big)\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "print(depth_big.min(), depth_big.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca531e0d-bee2-4c11-b4ae-a7ff8ff01728",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_path = root / scan_name / 'depth' / f'{frames[frame_ndx]}.png' \n",
    "# invert dims in the tensor\n",
    "# N, H, W -> torch nn convention\n",
    "depth = load_depth(depth_path, proj_img_size)\n",
    "print(depth.shape)\n",
    "plt.axis('off')\n",
    "plt.imshow(depth)\n",
    "plt.colorbar()\n",
    "print(depth.min(), depth.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06261b-af27-4328-8461-b1e40ce232f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# original RGB image\n",
    "rgb_path = root / scan_name / 'color' / f'{frames[frame_ndx]}.jpg' \n",
    "rgb = load_color(rgb_path, (320, 240))\n",
    "print(rgb.shape)\n",
    "plt.axis('off')\n",
    "plt.imshow(rgb.transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f0048c-4de4-45d4-be0e-097202831d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small RGB image\n",
    "rgb_path = root / scan_name / 'color' / f'{frames[frame_ndx]}.jpg' \n",
    "rgb = load_color(rgb_path, proj_img_size)\n",
    "print(rgb.shape)\n",
    "plt.axis('off')\n",
    "plt.imshow(rgb.transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d26b7-6d0d-4e61-8fd1-438d3e687f0b",
   "metadata": {},
   "source": [
    "## Draw correspondences between RGB image and scene/voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a73a22-43f7-488a-80ea-5bcd4febfc14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get projection\n",
    "intrinsic_path = root / scan_name / 'intrinsic/intrinsic_color.txt'\n",
    "intrinsic = load_intrinsic(intrinsic_path)\n",
    "# adjust for smaller image size\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], proj_img_size)\n",
    "\n",
    "projection = ProjectionHelper(\n",
    "            intrinsic, \n",
    "            0.4, 4.0,\n",
    "            proj_img_size,\n",
    "            subvol_size, voxel_size\n",
    "        )\n",
    "\n",
    "proj3d, proj2d = projection.compute_projection(torch.Tensor(depth), torch.Tensor(pose), torch.Tensor(w2g))\n",
    "print(proj3d.shape, proj2d.shape)\n",
    "num_inds = proj3d[0]\n",
    "print('Num correspondences:', proj3d[0], proj2d[0])\n",
    "ind3d = proj3d[1:1+num_inds]\n",
    "ind2d = proj2d[1:1+num_inds]\n",
    "print('3d ind range:',  (ind3d.min(), ind3d.max()), 'unique:', torch.unique(ind3d).shape, 'length:', torch.prod(torch.Tensor(subvol_size)))\n",
    "print(f'2d ind range: {ind2d.min(), ind2d.max()}', f'unique: {torch.unique(ind2d).shape}', 'length:', torch.prod(torch.Tensor(proj_img_size)))\n",
    "\n",
    "print('Occupied voxels', (subvol_x == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831cd01b-c206-4a5a-ba58-c7c7c04b2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ijk coordinates into the chunk (array indices)\n",
    "coords_3d = ProjectionHelper.lin_ind_to_coords_static(ind3d, subvol_size).T[:, :-1].long()\n",
    "print('3d coords:', coords_3d.shape, coords_3d.dtype)\n",
    "# viz in red\n",
    "colors = torch.zeros(num_inds, 3, dtype=int)\n",
    "colors[:, 0] = (torch.arange(num_inds) * 255 / num_inds).floor()\n",
    "print('colors:', colors.shape)\n",
    "print('Coords range', coords_3d.min(axis=0)[0], coords_3d.max(axis=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a67107-508b-4bf9-8084-3f09aaf7cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_2d = ProjectionHelper.lin_ind2d_to_coords2d_static(ind2d, proj_img_size).T.numpy()\n",
    "\n",
    "color_2d = np.zeros(proj_img_size[::-1] + (3,), dtype=int)\n",
    "color_2d[coords_2d[:, 1], coords_2d[:, 0]] = (255, 0, 0)\n",
    "plt.imshow(color_2d)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50979f-841c-4251-a450-00a4e6bed011",
   "metadata": {},
   "source": [
    "## draw occupied and mapped voxels with colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039136ec-7d9a-4ed4-a713-e5942361e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = trimesh.scene.scene.Scene() \n",
    "\n",
    "# scene.add_geometry(scan)\n",
    "\n",
    "# get the actual subvol\n",
    "x = subvol_x\n",
    "dims = x.shape\n",
    "\n",
    "print('Draw occupied voxels')\n",
    "for i in tqdm(range(dims[0])):\n",
    "    for j in range(dims[1]):\n",
    "        for k in range(dims[2]):\n",
    "            if x[i, j, k] == 1:\n",
    "                # get the transformation of this voxel\n",
    "                t = torch.eye(4)\n",
    "                t[:3, -1] = -torch.Tensor((i, j, k))\n",
    "                # add an additional translation to existing one\n",
    "                voxel_w2g = t @ w2g\n",
    "                voxel_g2w = torch.inverse(torch.Tensor(voxel_w2g)).numpy()\n",
    "                box = trimesh.creation.box(voxel_dims, voxel_g2w)\n",
    "                # make the box blue\n",
    "                box.visual.face_colors = np.zeros((12, 4)) + (0, 0, 255, 128)\n",
    "                box.visual.vertex_colors = np.zeros((8, 4)) + (0, 0, 255, 255)\n",
    "                scene.add_geometry(box)\n",
    "\n",
    "intersection = 0                \n",
    "print('Draw mapped voxels')\n",
    "for coord_3d, color in tqdm(zip(coords_3d, colors)): \n",
    "    i, j, k = coord_3d.tolist()\n",
    "    if x[i, j, k] == 1:\n",
    "        intersection += 1\n",
    "    # get the transformation of this voxel\n",
    "    # w2g is wrt center of the chunk, but ijk is wrt a corner of the chunk\n",
    "    # hence subtract half chunk size from ijk to get \"grid coord\"\n",
    "    t = torch.eye(4)\n",
    "    t[:3, -1] = -torch.Tensor((i, j, k))\n",
    "    # add an additional translation to existing one\n",
    "    voxel_w2g = t @ w2g\n",
    "    voxel_g2w = torch.inverse(torch.Tensor(voxel_w2g)).numpy()\n",
    "    box = trimesh.creation.box(voxel_dims, voxel_g2w)\n",
    "\n",
    "    color_tup = tuple(color.tolist())\n",
    "    box.visual.face_colors = np.zeros((12, 4)) + (255, 0, 0, 128)\n",
    "    box.visual.vertex_colors = np.zeros((8, 4)) + (255, 0, 0, 128)\n",
    "    scene.add_geometry(box)\n",
    "\n",
    "print(f'Mapped voxels: {len(coords_3d)}')\n",
    "print(f'Occupied voxels: {(x == 1).sum()}')\n",
    "print(f'Mapped and occupied voxels: {intersection} ')\n",
    "\n",
    "intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], (40, 30))\n",
    "\n",
    "pose_path = root / scan_name / 'pose' / f'{frames[frame_ndx]}.txt'\n",
    "pose = load_pose(pose_path).numpy()\n",
    "focal = (intrinsic[0, 0], intrinsic[1, 1])\n",
    "cam = trimesh.scene.Camera(name=f'{scan_name}_{frames[frame_ndx]}', resolution=(40, 30), focal=focal, z_near=0.4, z_far=4.0) \n",
    "camball, campath = trimesh.creation.camera_marker(cam, marker_height=4, origin_size=0.05)\n",
    "camball.apply_transform(pose)\n",
    "campath.apply_transform(pose)\n",
    "campath.colors = np.ones((5, 3)) * np.array((0, 255, 0))\n",
    "print('Add cam')\n",
    "scene.add_geometry(camball)\n",
    "scene.add_geometry(campath)\n",
    "\n",
    "axes = trimesh.creation.axis(axis_radius=0.1, axis_length=10)\n",
    "print('Add axes')\n",
    "# scene.add_geometry(axes)\n",
    "\n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f462f-1877-4d2d-b2de-c148f3aed260",
   "metadata": {},
   "source": [
    "## Viz correspondence b/w mapped voxels and pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d3a65-caed-4041-a20f-578584adf6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_pix(grid_ndx, w2g, cam2world, intrinsic, subvol_size):\n",
    "    '''\n",
    "    map grid(chunk) indices to pixel coordinates\n",
    "    '''\n",
    "    # convert ijk to grid coords, then grid to world and world to cam\n",
    "    grid_coords = grid_ndx - torch.Tensor(subvol_size)/2  \n",
    "    # convert to homogenous to use transformation matrix\n",
    "    grid_coords = torch.cat([grid_coords, torch.ones(len(grid_coords), 1)], dim=-1)\n",
    "    # apply g2w\n",
    "    world_coords = torch.Tensor(g2w) @ grid_coords.T\n",
    "    \n",
    "    world2cam = torch.inverse(cam2world)\n",
    "    cam_coords = world2cam @ world_coords\n",
    "    \n",
    "    p = cam_coords\n",
    "    \n",
    "    x2d = (p[0] * intrinsic[0][0]) / p[2] + intrinsic[0][2]\n",
    "    y2d = (p[1] * intrinsic[1][1]) / p[2] + intrinsic[1][2]\n",
    "\n",
    "    pixel_coords = torch.vstack([x2d, y2d]).T.floor().long().numpy()\n",
    "    \n",
    "    return pixel_coords    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32f6ab-fbcc-4751-aa0e-b25393a681fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first map the projected voxels to pixel coords and use a single color, should match the previous image\n",
    "print('Projected voxels:', coords_3d.shape)\n",
    "\n",
    "x = f['x'][ndx]\n",
    "dims = x.shape\n",
    "occ_coords = []\n",
    "for i in range(dims[0]):\n",
    "    for j in range(dims[1]):\n",
    "        for k in range(dims[2]):\n",
    "            if x[i,j,k] == 1:\n",
    "                occ_coords.append((i,j,k))\n",
    "occ_coords = torch.Tensor(occ_coords)\n",
    "print(f'Occupied voxels:', len(occ_coords))\n",
    "\n",
    "# read pose\n",
    "pose_path = root / scan_name / 'pose' / f'{frames[frame_ndx]}.txt'\n",
    "cam2world = load_pose(pose_path)\n",
    "# read intrinsic\n",
    "intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], proj_img_size)\n",
    "\n",
    "proj_pixelcoords = grid_to_pix(coords_3d, w2g, cam2world, intrinsic, subvol_size)\n",
    "valid_proj = (proj_pixelcoords[:, 0] >= 0) \\\n",
    "        * (proj_pixelcoords[:, 1] >= 0) \\\n",
    "        * (proj_pixelcoords[:, 0] < proj_img_size[0]) \\\n",
    "        * (proj_pixelcoords[:, 1] < proj_img_size[1])\n",
    "proj_pixelcoords = proj_pixelcoords[valid_proj]\n",
    "print('Num valid proj pixels', proj_pixelcoords.shape)\n",
    "print('Proj Pixel coords range', proj_pixelcoords.min(axis=0), proj_pixelcoords.max(axis=0))\n",
    "\n",
    "occ_pixelcoords = grid_to_pix(occ_coords, w2g, cam2world, intrinsic, subvol_size)\n",
    "valid_occ = (occ_pixelcoords[:, 0] >= 0) \\\n",
    "        * (occ_pixelcoords[:, 1] >= 0) \\\n",
    "        * (occ_pixelcoords[:, 0] < proj_img_size[0]) \\\n",
    "        * (occ_pixelcoords[:, 1] < proj_img_size[1])\n",
    "occ_pixelcoords = occ_pixelcoords[valid_occ]\n",
    "print('Num valid occ pixels', occ_pixelcoords.shape)\n",
    "print('Occ Pixel coords range', occ_pixelcoords.min(axis=0), occ_pixelcoords.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a5f8b-4c4a-43b6-9d95-4ce73cb7bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the mapped voxels in 2d\n",
    "vox_2d = np.zeros(proj_img_size[::-1] + (3,), dtype=np.uint8)\n",
    "# Y,X / H,W indexing\n",
    "vox_2d[proj_pixelcoords[:, 1], proj_pixelcoords[:, 0]] = (255, 0, 0)\n",
    "plt.imshow(vox_2d)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5c165-4490-49ca-a115-2e2f7052f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the occupied voxels in 2d\n",
    "occ_2d = np.zeros(proj_img_size[::-1] + (3,), dtype=np.uint8)\n",
    "# Y,X / H,W indexing\n",
    "occ_2d[occ_pixelcoords[:, 1], occ_pixelcoords[:, 0]] = np.array((0, 0, 255), dtype=int)\n",
    "plt.imshow(occ_2d)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18999b05-34f2-4f57-8c77-bb80110fbc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viz both in 1 image\n",
    "both_2d = np.zeros(proj_img_size[::-1] + (3,), dtype=np.uint8)\n",
    "# Y,X / H,W indexing\n",
    "both_2d[occ_pixelcoords[:, 1], occ_pixelcoords[:, 0]] = np.array((0, 0, 255), dtype=int)\n",
    "both_2d[proj_pixelcoords[:, 1], proj_pixelcoords[:, 0]] = (255, 0, 0)\n",
    "plt.imshow(both_2d)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7917193d-5e94-45bc-8f50-f76cb91e0a93",
   "metadata": {},
   "source": [
    "## Draw point cloud from the camera pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df6344-b3be-46b3-8270-6395b2160999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def world_to_pix(world_coords, cam2world, intrinsic, subvol_size, img_size):\n",
    "    '''\n",
    "    map grid(chunk) indices to pixel coordinates\n",
    "    img_size: W, H\n",
    "    '''\n",
    "    world_coords = torch.cat([world_coords, torch.ones(len(world_coords), 1)], dim=-1).T\n",
    "    world2cam = torch.inverse(cam2world)\n",
    "    cam_coords = world2cam @ world_coords\n",
    "    \n",
    "    p = cam_coords\n",
    "    \n",
    "    x2d = (p[0] * intrinsic[0][0]) / p[2] + intrinsic[0][2]\n",
    "    y2d = (p[1] * intrinsic[1][1]) / p[2] + intrinsic[1][2]\n",
    "\n",
    "    # coords within image\n",
    "    valid_pix = (x2d >= 0) \\\n",
    "                * (y2d >= 0) \\\n",
    "                * (x2d <= img_size[0]) \\\n",
    "                * (y2d <= img_size[1])\n",
    "    # pick only positive depth\n",
    "    depth_mask = p[2] > 0\n",
    "    \n",
    "    mask = depth_mask * valid_pix\n",
    "    \n",
    "    x2d, y2d = x2d[mask], y2d[mask]\n",
    "    pixel_coords = torch.vstack([x2d, y2d]).T.floor().long().numpy()\n",
    "    \n",
    "    return pixel_coords, mask   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b12fe-ca8a-4f39-942f-3170a864f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vertices', scan.vertices.shape, scan.visual.vertex_colors.shape)\n",
    "\n",
    "# read pose\n",
    "pose_path = root / scan_name / 'pose' / f'{frames[frame_ndx]}.txt'\n",
    "cam2world = load_pose(pose_path)\n",
    "# read intrinsic\n",
    "intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], proj_img_size)\n",
    "\n",
    "print(scan.vertices.min(axis=0), scan.vertices.max(axis=0))\n",
    "\n",
    "pc_pixelcoords, mask = world_to_pix(torch.Tensor(scan.vertices), cam2world, intrinsic, subvol_size, proj_img_size)\n",
    "print('Num PC pixels', pc_pixelcoords.shape)\n",
    "print('PC Pixel coords range', pc_pixelcoords.min(axis=0), pc_pixelcoords.max(axis=0))\n",
    "pc_colors = scan.visual.vertex_colors[mask][:, :3]\n",
    "print('New colors', pc_colors.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e6d0e-b395-4ed7-abb4-d99e93a1dc4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the point cloud (vertices) from the chosen pose in 2d\n",
    "pc_2d = np.ones(proj_img_size[::-1] + (3,), dtype=np.uint8) * 255\n",
    "# Y,X / H,W indexing\n",
    "pc_2d[pc_pixelcoords[:, 1], pc_pixelcoords[:, 0]] = pc_colors\n",
    "plt.imshow(pc_2d)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d8528a-249b-47c6-bc6b-83b3ed89484d",
   "metadata": {},
   "source": [
    "## Draw one chunk and 2 RGBs\n",
    "- Project 2 different frames to the same chunk\n",
    "- Pick the voxels that are seen by both images\n",
    "- Draw the chunk as seen by the 1st image, and draw correspondences to both images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43dc996-d1d0-4059-a42a-5a59b438e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_everything(scan_name, frame, proj_img_size):\n",
    "    pose_path = root / scan_name / 'pose' / f'{frame}.txt'\n",
    "    pose = load_pose(pose_path)\n",
    "    depth_path = root / scan_name / 'depth' / f'{frame}.png' \n",
    "    depth = load_depth(depth_path, proj_img_size)\n",
    "    rgb_path = root / scan_name / 'color' / f'{frame}.jpg' \n",
    "    rgb = load_color(rgb_path, proj_img_size)\n",
    "    \n",
    "    return pose, depth, rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9aa764-1bea-4487-a166-7affb84259cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_corresp_multiview(x, frame1, frame2, scan_name, proj_img_size, w2g, common_img, match_inds=None, n_matches=None):\n",
    "    pose1, depth1, rgb1 = load_everything(scan_name, frame1, proj_img_size)\n",
    "    pose2, depth2, rgb2 = load_everything(scan_name, frame2, proj_img_size)\n",
    "    intrinsic_path = root / scan_name / 'intrinsic/intrinsic_color.txt'\n",
    "    intrinsic = load_intrinsic(intrinsic_path)\n",
    "    # adjust for smaller image size\n",
    "    intrinsic = adjust_intrinsic(intrinsic, [1296, 968], proj_img_size)\n",
    "    \n",
    "    projection = ProjectionHelper(\n",
    "            intrinsic, \n",
    "            0.4, 4.0,\n",
    "            proj_img_size,\n",
    "            subvol_size, voxel_size\n",
    "        )\n",
    "    t = torch.eye(4)\n",
    "    t[:3, -1] = 16\n",
    "    w2g_tmp = t @ w2g\n",
    "    \n",
    "    proj3d1, proj2d1 = projection.compute_projection(torch.Tensor(depth1), torch.Tensor(pose1), torch.Tensor(w2g_tmp))\n",
    "    num_inds1 = proj3d1[0]\n",
    "    ind3d1 = proj3d1[1:1+num_inds1]\n",
    "    ind2d1 = proj2d1[1:1+num_inds1]\n",
    "    \n",
    "    proj3d2, proj2d2 = projection.compute_projection(torch.Tensor(depth2), torch.Tensor(pose2), torch.Tensor(w2g_tmp))\n",
    "    num_inds2 = proj3d2[0]\n",
    "    ind3d2 = proj3d2[1:1+num_inds2]\n",
    "    ind2d2 = proj2d2[1:1+num_inds2]\n",
    "    \n",
    "    # voxels seen by rgb1\n",
    "    coords_3d1 = torch.empty(4, num_inds1)\n",
    "    coords_3d1 = ProjectionHelper.lin_ind_to_coords_static(ind3d1, coords_3d1, subvol_size).T[:, :-1].long()            \n",
    "    # pick the ones that are seen by rgb2 as well\n",
    "    proj_pixelcoords2 = grid_to_pix(coords_3d1, w2g, pose2, intrinsic, subvol_size)\n",
    "    # should lie within the image1\n",
    "    valid_in2 = (proj_pixelcoords2[:, 0] >= 0) \\\n",
    "            * (proj_pixelcoords2[:, 1] >= 0) \\\n",
    "            * (proj_pixelcoords2[:, 0] < proj_img_size[0]) \\\n",
    "            * (proj_pixelcoords2[:, 1] < proj_img_size[1])\n",
    "    proj_pixelcoords2 = proj_pixelcoords2[valid_in2]\n",
    "    \n",
    "    # pick only these points in rgb1\n",
    "    proj_pixelcoords1 = grid_to_pix(coords_3d1, w2g, pose1, intrinsic, subvol_size)\n",
    "    proj_pixelcoords1 = proj_pixelcoords1[valid_in2]\n",
    "    ######################################################################\n",
    "    # voxel indices that are occupied\n",
    "    dims = x.shape\n",
    "    occ_coords = []\n",
    "    for i in range(dims[0]):\n",
    "        for j in range(dims[1]):\n",
    "            for k in range(dims[2]):\n",
    "                if x[i,j,k] == 1:\n",
    "                    occ_coords.append((i,j,k))\n",
    "    occ_coords = torch.Tensor(occ_coords)\n",
    "    # corresponding pixel coords\n",
    "    occ_pixelcoords = grid_to_pix(occ_coords, w2g, pose1, intrinsic, subvol_size)\n",
    "    # should lie within the image1\n",
    "    valid_occ_in1 = (occ_pixelcoords[:, 0] >= 0) \\\n",
    "            * (occ_pixelcoords[:, 1] >= 0) \\\n",
    "            * (occ_pixelcoords[:, 0] < proj_img_size[0]) \\\n",
    "            * (occ_pixelcoords[:, 1] < proj_img_size[1])\n",
    "    occ_pixelcoords = occ_pixelcoords[valid_occ_in1]\n",
    "    \n",
    "    # 2d image with occupied voxels\n",
    "    occ_2d1 = np.zeros(proj_img_size[::-1] + (3,), dtype=np.uint8)\n",
    "    occ_2d1[occ_pixelcoords[:, 1], occ_pixelcoords[:, 0]] = np.array((0, 0, 255), dtype=int)\n",
    "    ######################################################################\n",
    "    # the set of common voxels to show in each rgb\n",
    "    if n_matches:\n",
    "        # pick matches\n",
    "        match_inds = torch.randperm(len(proj_pixelcoords2))[:n_matches]\n",
    "    else:\n",
    "        # match indices are supplied\n",
    "        assert match_inds is not None\n",
    "        n_matches = len(match_inds)\n",
    "    matches = torch.arange(n_matches).repeat(2, 1).T\n",
    "    ######################################################################\n",
    "    # viz1\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    match_coords1 = proj_pixelcoords1[match_inds]\n",
    "    # interchange X and Y\n",
    "    match_coords1 = np.vstack([match_coords1[:, 1], match_coords1[:, 0]]).T\n",
    "\n",
    "    plot_matches(ax, np.transpose(rgb1, (1, 2, 0)), common_img, match_coords1, match_coords1, matches,\n",
    "                 only_matches=False, keypoints_color='white', matches_color='red')\n",
    "    ######################################################################\n",
    "    # viz2\n",
    "    match_coords2 = proj_pixelcoords2[match_inds]\n",
    "    # interchange X and Y\n",
    "    match_coords2 = np.vstack([match_coords2[:, 1], match_coords2[:, 0]]).T\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.axis('off')\n",
    "    plot_matches(ax, np.transpose(rgb2, (1, 2, 0)), common_img, match_coords2, match_coords1, matches,\n",
    "                 only_matches=False, keypoints_color='white', matches_color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8bc1d2-fc89-412d-a065-c0e3807c40f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1, frame2 = frames[0], frames[2]\n",
    "draw_corresp_multiview(x, frame1, frame2, scan_name, proj_img_size, w2g, scene_with_voxels_rgb, match_inds=match_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706bae7-0a80-4cca-b7d3-894bb9a7f176",
   "metadata": {},
   "source": [
    "## Draw matches between RGB and rendered PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15c050-9a6d-4ea5-bf55-8e6913b37c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.axis('off')\n",
    "# select a few correspondences to draw\n",
    "n_matches = 8\n",
    "match_inds = torch.randperm(len(proj_pixelcoords))[:n_matches]\n",
    "match_coords = proj_pixelcoords[match_inds]\n",
    "# interchange X and Y\n",
    "match_coords = np.vstack([match_coords[:, 1], match_coords[:, 0]]).T\n",
    "matches = torch.arange(n_matches).repeat(2, 1).T\n",
    "\n",
    "plot_matches(ax, np.transpose(rgb, (1, 2, 0)), overlay_img, match_coords, match_coords.copy(), matches,\n",
    "             only_matches=False, keypoints_color='white', matches_color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd93125-94db-4a0d-8b1c-15589bb53e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(proj_pixelcoords), len(coords_3d), len(colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ecb4b-9a49-40f9-96a6-6a95c5d51bef",
   "metadata": {},
   "source": [
    "### Draw only the selected matched voxels in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5167b3a1-015d-44db-9ba5-d85dc2bb5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_3d_valid = coords_3d[valid_proj]\n",
    "coords_3d_viz = coords_3d_valid[match_inds]\n",
    "\n",
    "rotx = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, -1, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [ 0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "roty = np.array([\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [-1, 0, 0, 0],\n",
    "    [ 0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "rotz = np.array([\n",
    "    [0, -1, 0, 0],\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [ 0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "# # rotx\n",
    "pose_tmp = pose_fixed @ rotz #@ rotz\n",
    "\n",
    "cam = trimesh.scene.Camera(name=f'{scan_name}_{frames[cam_ndx]}', resolution=proj_img_size, focal=render_focal, z_near=0.4, z_far=4.0)\n",
    "scene = trimesh.scene.scene.Scene(camera=cam, camera_transform=pose_fixed) \n",
    "\n",
    "print('Draw subset of mapped voxels')\n",
    "for coord_3d, color in tqdm(zip(coords_3d_viz, colors)): \n",
    "    i, j, k = coord_3d.tolist()\n",
    "    # get the transformation of this voxel\n",
    "    # w2g is wrt center of the chunk, but ijk is wrt a corner of the chunk\n",
    "    # hence subtract half chunk size from ijk to get \"grid coord\"\n",
    "    t = torch.eye(4)\n",
    "    t[:3, -1] = -torch.Tensor((i, j, k))\n",
    "    # add an additional translation to existing one\n",
    "    voxel_w2g = t @ w2g\n",
    "    voxel_g2w = torch.inverse(torch.Tensor(voxel_w2g)).numpy()\n",
    "    box = trimesh.creation.box(voxel_dims, voxel_g2w)\n",
    "\n",
    "    color_tup = tuple(color.tolist())\n",
    "    box.visual.face_colors = np.zeros((12, 4)) + ((255, 0, 0, 255))\n",
    "    box.visual.vertex_colors = np.zeros((8, 4)) + ((255, 0, 0, 255))\n",
    "    scene.add_geometry(box)\n",
    "\n",
    "# scene.add_geometry(scan)\n",
    "    \n",
    "scene.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84980fb8-7697-4942-9026-73aafb722664",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_fixed@roty, pose_fixed@rotz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821042ec-aa68-4a2c-acb0-5396b75d34c9",
   "metadata": {},
   "source": [
    "### render mesh+few voxels from camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63767e4a-0dbd-490c-b02a-95c3fdaf8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyscene = pyrender.Scene()\n",
    "voxels_mesh = scene.dump(concatenate=True)\n",
    "\n",
    "pyscene.add(pyrender.Mesh.from_trimesh(voxels_mesh, smooth=False))\n",
    "pyscene.add(pyrender.Mesh.from_trimesh(scan))\n",
    "pyscene.add(render_cam, pose=pose_fixed)\n",
    "pyscene.add(render_light, pose=pose_fixed)\n",
    "\n",
    "scene_with_voxels_rgb, _ = r.render(pyscene)\n",
    "print(scene_with_voxels_rgb.shape)\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(12, 9)\n",
    "plt.imshow(scene_with_voxels_rgb)\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a6eea6-412b-4d83-b24d-168b0a9ebaaa",
   "metadata": {},
   "source": [
    "### Draw correspondences RGB--mesh+voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9467af50-c946-44d4-829a-af2e2bd3cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.axis('off')\n",
    "plot_matches(ax, np.transpose(rgb, (1, 2, 0)), scene_with_voxels_rgb, match_coords, match_coords.copy(), matches,\n",
    "             only_matches=False, keypoints_color='white', matches_color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4583a39-032f-4dab-91f3-1a7b6c735e73",
   "metadata": {},
   "source": [
    "## Draw all the cameras in a scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508b34f-b9ad-468f-8116-be49a323b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw all cameras in a scene\n",
    "scene = trimesh.scene.scene.Scene() \n",
    "\n",
    "scan_name = get_scan_name(sceneid, scanid)\n",
    "print(scan_name)\n",
    "root = Path('/mnt/data/scannet/scans')\n",
    "scan_path = root / f'{scan_name}/{scan_name}_vh_clean_2.ply'\n",
    "scan = trimesh.load(scan_path)\n",
    "print(scan)\n",
    "print('Add scan')\n",
    "scene.add_geometry(scan)\n",
    "\n",
    "box = trimesh.creation.box(subvol_size, g2w)\n",
    "print('Add box')\n",
    "scene.add_geometry(box)\n",
    "\n",
    "intrinsic = make_intrinsic(1170.187988, 1170.187988, 647.75, 483.75)\n",
    "intrinsic = adjust_intrinsic(intrinsic, [1296, 968], (40, 30))\n",
    "pose_files = sorted(os.listdir(root / scan_name / 'pose'), key=lambda f: int(osp.splitext(f)[0]))\n",
    "pose_indices = range(0, len(pose_files), frame_skip)\n",
    "print('Add poses:', len(pose_indices))\n",
    "frame_skip = 40\n",
    "focal = (intrinsic[0, 0], intrinsic[1, 1]) \n",
    "for ndx, pose_ndx in enumerate(tqdm(pose_indices)):\n",
    "    pose_path = root / scan_name / 'pose' / pose_files[pose_ndx]\n",
    "    pose = load_pose(pose_path).numpy()\n",
    "    \n",
    "    cam = trimesh.scene.Camera(name=f'{scan_name}_{frames[0]}', resolution=(40, 30), focal=focal, z_near=0.4, z_far=4.0) \n",
    "    camball, campath = trimesh.creation.camera_marker(cam, marker_height=4, origin_size=0.05)\n",
    "    \n",
    "    camball.apply_transform(pose)\n",
    "    campath.apply_transform(pose)\n",
    "    campath.colors = np.zeros((5, 3))\n",
    "    scene.add_geometry(camball)\n",
    "    scene.add_geometry(campath)\n",
    "\n",
    "# draw the best pose in a different color\n",
    "pose_path = root / scan_name / 'pose' / f'{frames[0]}.txt'\n",
    "pose = load_pose(pose_path).numpy()\n",
    "focal = (intrinsic[0, 0], intrinsic[1, 1])\n",
    "cam = trimesh.scene.Camera(name=f'{scan_name}_{frames[0]}', resolution=(40, 30), focal=focal, z_near=0.4, z_far=4.0) \n",
    "camball, campath = trimesh.creation.camera_marker(cam, marker_height=4, origin_size=0.05)\n",
    "camball.apply_transform(pose)\n",
    "campath.apply_transform(pose)\n",
    "# green\n",
    "campath.colors = np.ones((5, 3)) * np.array((0, 255, 0))\n",
    "print('Add cam')\n",
    "scene.add_geometry(camball)\n",
    "scene.add_geometry(campath)\n",
    "\n",
    "axes = trimesh.creation.axis(axis_radius=0.1, axis_length=10)\n",
    "print('Add axes')\n",
    "scene.add_geometry(axes)\n",
    "\n",
    "scene.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
